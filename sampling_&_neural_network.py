# -*- coding: utf-8 -*-
"""sampling & neural network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Plv4EZIskzlfzNNq6EOyEo45Gv5-NF0k

# **Imports**
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.nn.functional as F

import matplotlib.pyplot as plt
import time
import random
import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings("ignore")

"""# **Preprocessing**"""

def preprocessing_features(df_train):
    """
    parse features column from long string to columns of features for every packet
    """
    first_drop = pd.DataFrame(df_train['feature'].str.split('['))
    x = pd.DataFrame(first_drop.feature.values.tolist()).add_prefix('feature_')
    x.drop('feature_0', axis=1, inplace=True)
    second_drop = pd.DataFrame(x['feature_1'].str.split(']'))
    x = pd.DataFrame(second_drop.feature_1.values.tolist()).add_prefix('feature_')
    x.drop('feature_1', axis=1, inplace=True)
    x = x.rename({'feature_0':'feature'}, axis=1)
    features = pd.DataFrame(x['feature'].str.split(','))
    x = pd.DataFrame(features.feature.values.tolist()).add_prefix('feature_')
    x = x.fillna(0)
    x = x.apply(np.float64)
    return x

# get the data from csv

csvfile = pd.read_csv('/content/drive/MyDrive/combine.csv')
df = csvfile.sample(25000)

print(df['app_label'].unique())

#### todo - count the number of uniques with FM

from torch.utils.data import Dataset

class PcapDataset(Dataset):
    def __init__(self, df_train):
      x = preprocessing_features(df_train)
      y = df_train['app_label']
      y = y.apply(np.float64)
      y.replace('0', 0, inplace=True)
      y.replace('1', 1, inplace=True)
      y.replace('2', 2, inplace=True)
      y.replace('3', 3, inplace=True)
      y.replace('4', 4, inplace=True)
      y.replace('5', 5, inplace=True)
      y.replace('6', 6, inplace=True)
      y.replace('7', 7, inplace=True)
      y.replace('8', 8, inplace=True)
      y.replace('9', 9, inplace=True)
      y.replace('10', 10, inplace=True)
      y.replace('11', 11, inplace=True)
      y.replace('12', 12, inplace=True)
      y.replace('13', 13, inplace=True)
      y.replace('15', 15, inplace=True)
      y.replace('16', 14, inplace=True)

      self.x_train = torch.tensor(x.values, dtype=torch.float64)
      self.y_train = torch.tensor(y.values, dtype=torch.float64)

    def __len__(self):
        return len(self.y_train)

    def __getitem__(self, idx):
        return self.x_train[idx], self.y_train[idx]

# train-test split
df['split'] = np.random.randn(df.shape[0], 1)
msk = np.random.rand(len(df)) <= 0.8
train = df[msk]
test = df[~msk]
train.drop(columns=['split'], inplace=True)
test.drop(columns=['split'], inplace=True)
train.dropna(axis=0, how='any', inplace=True)
test.dropna(axis=0, how='any', inplace=True)

# load data
pcap_train = PcapDataset(train)
print(pcap_train.y_train.unique())
pcap_test = PcapDataset(test)
print(pcap_test.y_train.unique())

pcap_train.y_train[pcap_train.y_train==16] = 4
pcap_test.y_train[pcap_test.y_train==16] = 4
pcap_train.y_train[pcap_train.y_train==12] = 7
pcap_test.y_train[pcap_test.y_train==12] = 7
pcap_train.y_train[pcap_train.y_train==10] = 8
pcap_test.y_train[pcap_test.y_train==10] = 8

print(pcap_train.y_train.unique())
print(pcap_test.y_train.unique())

train_loader = torch.utils.data.DataLoader(pcap_train, batch_size=25, shuffle=True, drop_last=True)
test_loader = torch.utils.data.DataLoader(pcap_test, batch_size=25, shuffle=False, drop_last=True)

"""# **CNN - train and evaluate model on original data** (packets before sampling)"""

class CNN(nn.Module):

  def __init__(self):
      super(CNN, self).__init__()
      self.conv1 = nn.Conv1d(1, 200, 5, 1, 0)
      self.conv2 = nn.Conv1d(200, 100, 5, 1, 0)

      self.dropout = nn.Dropout(p=0.05)

      self.fc1 = nn.Linear(37200, 10000)
      self.fc2 = nn.Linear(10000, 2000)
      self.fc3 = nn.Linear(2000, 500)
      self.fc4 = nn.Linear(500, 100)
      self.out = nn.Linear(100,10)

  def forward(self, x):
      x = F.avg_pool1d(F.relu(self.conv1(x)),2)
      x = self.dropout(x)
      x = F.avg_pool1d(F.relu(self.conv2(x)),2)
      x = self.dropout(x)

      x = torch.flatten(x, start_dim=1)
      x = F.relu(self.fc1(x))
      x = self.dropout(x)     
      x = F.relu(self.fc2(x))
      x = self.dropout(x)
      x = F.relu(self.fc3(x))
      x = self.dropout(x)
      x = F.relu(self.fc4(x))
      x = self.dropout(x)

      x = self.out(x)

      return x

model = CNN().cuda()
print(model)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# Train the model
train_loss = []
interval_tuples = []
start = time.time()

for epoch in range(5):
    running_train_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        labels = labels.long()
        inputs = np.expand_dims(inputs, axis=1)
        inputs = torch.Tensor(inputs)
        inputs = inputs.cuda() # -- for GPU
        labels = labels.cuda() # -- for GPU
        
        # zero the parameters gradients
        optimizer.zero_grad()

        # forward + backward + optimization
        outputs = model(inputs)

        #print(outputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        # print statistics
        running_train_loss += loss.item()
        if (i+1) % 100 == 0:
            interval_tuples.append(str((epoch + 1, i + 1)))
            print("[{}, {}] loss: {}".format(epoch + 1, i + 1, running_train_loss / 100))
            train_loss.append(running_train_loss / 100)
            running_train_loss = 0.0

stop = time.time()
original_time = stop - start
print("Training time: {}".format(original_time))

total = 0
correct = 0
with torch.no_grad(): 
    for data in test_loader:
        inputs, labels = data
        labels = labels.float()
        inputs = np.expand_dims(inputs, axis=1)
        inputs = torch.Tensor(inputs)
        inputs = inputs.cuda() # -- for GPU
        labels = labels.cuda() # -- for GPU

        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
original_accuracy = (100 * correct / total)
print('Accuracy: {}%'.format(original_accuracy))

"""# **Packets' sampling experiments**"""

def trimmed_packet(df, trimmed_length):
    # Systematic sampling - trimming the packet (first trimmed_length_bytes)
    return df[df.columns[0:trimmed_length]] 

def every_other_x_packet(df, x):
    # Systematic sampling - choosing every other xth element of the packet
    return df[df.columns[::x]]

def random_sampled_packet(df, sampled_length):
    # Random sampling (reservoir)
    i = 0
    r = np.zeros((df.shape[0],sampled_length))
    for i in range(df.shape[0]):
      for j in range(sampled_length):
          r[i,j] = df.iloc[i,j]
      for j in range(sampled_length,df.shape[1]):
          l = random.randrange(j + 1)
          if l < sampled_length:
              k = random.randrange(sampled_length)
              r[i,k] = df.iloc[i,k]
    return pd.DataFrame(r)

df_train = pd.DataFrame(pcap_train.x_train.numpy())
df_test = pd.DataFrame(pcap_test.x_train.numpy())

# 50%

random_train = random_sampled_packet(df_train, 750)
random_test = random_sampled_packet(df_test, 750)

trimmed_train = trimmed_packet(df_train, 750)
trimmed_test = trimmed_packet(df_test, 750)

everyother_train = every_other_x_packet(df_train, 2)
everyother_test = every_other_x_packet(df_test, 2)

# 20%

random_train = random_sampled_packet(df_train, 300)
random_test = random_sampled_packet(df_test, 300)

trimmed_train = trimmed_packet(df_train, 300)
trimmed_test = trimmed_packet(df_test, 300)

everyother_train = every_other_x_packet(df_train, 5)
everyother_test = every_other_x_packet(df_test, 5)

# 10%

random_train = random_sampled_packet(df_train, 150)
random_test = random_sampled_packet(df_test, 150)


trimmed_train = trimmed_packet(df_train, 150)
trimmed_test = trimmed_packet(df_test, 150)

everyother_train = every_other_x_packet(df_train, 10)
everyother_test = every_other_x_packet(df_test, 10)

# 5%

random_train = random_sampled_packet(df_train, 75)
random_test = random_sampled_packet(df_test, 75)

trimmed_train = trimmed_packet(df_train, 75)
trimmed_test = trimmed_packet(df_test, 75)

everyother_train = every_other_x_packet(df_train, 20)
everyother_test = every_other_x_packet(df_test, 20)

# 3%

random_train = random_sampled_packet(df_train, 45)
random_test = random_sampled_packet(df_test, 45)

trimmed_train = trimmed_packet(df_train, 45)
trimmed_test = trimmed_packet(df_test, 45)

everyother_train = every_other_x_packet(df_train, 33)
everyother_test = every_other_x_packet(df_test, 33)

# 1%

random_train = random_sampled_packet(df_train, 15)
random_test = random_sampled_packet(df_test, 15)

trimmed_train = trimmed_packet(df_train, 15)
trimmed_test = trimmed_packet(df_test, 15)

everyother_train = every_other_x_packet(df_train, 100)
everyother_test = every_other_x_packet(df_test, 100)

random_train_tensor = torch.tensor(random_train.values)
random_test_tensor = torch.tensor(random_test.values)

trimmed_train_tensor = torch.tensor(trimmed_train.values)
trimmed_test_tensor = torch.tensor(trimmed_test.values)

everyother_train_tensor = torch.tensor(everyother_train.values)
everyother_test_tensor = torch.tensor(everyother_test.values)

#pcap_train.x_train = random_train_tensor
#pcap_test.x_train = random_test_tensor

#pcap_train.x_train = trimmed_train_tensor
#pcap_test.x_train = trimmed_test_tensor

pcap_train.x_train = everyother_train_tensor
pcap_test.x_train = everyother_test_tensor

train_loader = torch.utils.data.DataLoader(pcap_train, batch_size=25, shuffle=True, drop_last=True)
test_loader = torch.utils.data.DataLoader(pcap_test, batch_size=25, shuffle=False, drop_last=True)

class CNN(nn.Module):

  def __init__(self):
      super(CNN, self).__init__()
      self.conv1 = nn.Conv1d(1, 200, 5, 1, 0)
      self.conv2 = nn.Conv1d(200, 100, 5, 1, 0)

      self.dropout = nn.Dropout(p=0.05)

      self.fc1 = nn.Linear(100, 10000)
      self.fc2 = nn.Linear(10000, 2000)
      self.fc3 = nn.Linear(2000, 500)
      self.fc4 = nn.Linear(500, 100)
      self.out = nn.Linear(100,10)

  def forward(self, x):
      x = F.avg_pool1d(F.relu(self.conv1(x)),2)
      x = self.dropout(x)
      #x = F.avg_pool1d(F.relu(self.conv2(x)),2)
      x = F.relu(self.conv2(x))
      x = self.dropout(x)

      x = torch.flatten(x, start_dim=1)
      #print(x.shape)
      x = F.relu(self.fc1(x))
      x = self.dropout(x)     
      x = F.relu(self.fc2(x))
      x = self.dropout(x)
      x = F.relu(self.fc3(x))
      x = self.dropout(x)
      x = F.relu(self.fc4(x))
      x = self.dropout(x)

      x = self.out(x)

      return x

model = CNN().cuda()
print(model)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# Train the model
train_loss = []
interval_tuples = []
start = time.time()

for epoch in range(5):
    running_train_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        labels = labels.long()
        inputs = np.expand_dims(inputs, axis=1)
        inputs = torch.Tensor(inputs)
        inputs = inputs.cuda() # -- for GPU
        labels = labels.cuda() # -- for GPU
        
        # zero the parameters gradients
        optimizer.zero_grad()

        # forward + backward + optimization
        outputs = model(inputs)

        #print(outputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        # print statistics
        running_train_loss += loss.item()
        if (i+1) % 100 == 0:
            interval_tuples.append(str((epoch + 1, i + 1)))
            print("[{}, {}] loss: {}".format(epoch + 1, i + 1, running_train_loss / 100))
            train_loss.append(running_train_loss / 100)
            running_train_loss = 0.0

stop = time.time()
print("Training time: {}".format(stop-start))

total = 0
correct = 0
with torch.no_grad(): 
    for data in test_loader:
        inputs, labels = data
        labels = labels.float()
        inputs = np.expand_dims(inputs, axis=1)
        inputs = torch.Tensor(inputs)
        inputs = inputs.cuda() # -- for GPU
        labels = labels.cuda() # -- for GPU

        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print('Accuracy: {}%'.format((100 * correct / total)))

"""**Results' plot**"""

plt.figure(figsize=[15,10])
plt.plot([0.01, 0.03, 0.05, 0.1, 0.2, 0.5], [80.18, 97.24, 98.25, 97.5, 95.68, 94.21],
         [0.01, 0.03, 0.05, 0.1, 0.2, 0.5], [84.74, 95.91, 95.72, 95.36, 96.87, 95.49],
         [0.01, 0.03, 0.05, 0.1, 0.2, 0.5], [37.39, 42.16, 65.98, 82.02, 88.37, 92.64])
plt.xticks([0.01, 0.03, 0.05, 0.1, 0.2, 0.5])
plt.hlines(93.47, 0.01, 0.5, linestyle='dashed', color='r')
plt.legend(['random', 'trimming', 'every other', 'original accuracy'], prop={'size': 20})
plt.ylim((35,100))
plt.xlabel('Sample ratio', fontsize=20)
plt.ylabel('Accuracy', fontsize=20)

plt.figure(figsize=[15,10])
plt.plot([0.01, 0.03, 0.05, 0.1, 0.2, 0.5], [37, 47, 58, 85, 140, 304])
plt.xticks([0.01, 0.03, 0.05, 0.1, 0.2, 0.5])
plt.ylim((30,620))
plt.hlines(596, 0.01, 0.5, linestyle='dashed', color='r')
plt.legend(['average training time', 'original training time'], prop={'size': 20}, loc='lower right')
plt.xlabel('Sample ratio', fontsize=20)
plt.ylabel('Training time', fontsize=20)

"""## **Dimensionality Reduction Experiments**"""

class CNN_JL(nn.Module):

  def __init__(self, dim):
      super(CNN_JL, self).__init__()
      self.conv1 = nn.Conv1d(1, 200, 5, 1, 0)
      self.conv2 = nn.Conv1d(200, 100, 5, 1, 0)

      self.dropout = nn.Dropout(p=0.05)

      self.fc1 = nn.Linear(dim, 10000)
      self.fc2 = nn.Linear(10000, 2000)
      self.fc3 = nn.Linear(2000, 500)
      self.fc4 = nn.Linear(500, 100)
      self.out = nn.Linear(100,10)

  def forward(self, x):
      x = F.avg_pool1d(F.relu(self.conv1(x)),2)
      x = self.dropout(x)
      x = F.avg_pool1d(F.relu(self.conv2(x)),2)
    #   x = F.relu(self.conv2(x))
      x = self.dropout(x)

      x = torch.flatten(x, start_dim=1)
      x = F.relu(self.fc1(x))
      x = self.dropout(x)     
      x = F.relu(self.fc2(x))
      x = self.dropout(x)
      x = F.relu(self.fc3(x))
      x = self.dropout(x)
      x = F.relu(self.fc4(x))
      x = self.dropout(x)

      x = self.out(x)

      return x

df_train = pd.DataFrame(pcap_train.x_train.numpy())
df_test = pd.DataFrame(pcap_test.x_train.numpy())

from sklearn.random_projection import johnson_lindenstrauss_min_dim

# get list of components for each epsilon in range 0,1 to 0.9
components = []
for eps in list(np.linspace(0.1, 0.9, 9)):
    components.append(johnson_lindenstrauss_min_dim(1500,eps))
components = pd.Series(components)
components = components[components < 1500].sort_values(ascending=True)
components = list(components)
print(components)

# Using Johnson-Lindenstrauss dimensionality reduction technique
acc_jl_list = []
from sklearn.random_projection import SparseRandomProjection
# train model and get predictions
for dim in components:
    # dimensionality reduction
    # create the transformation
    sp = SparseRandomProjection(n_components = dim)
    x_train_np = sp.fit_transform(df_train)
    x_train_pd = pd.DataFrame(x_train_np)

    # evaluate the model and update the list of accuracies
    x_test_np = sp.transform(df_test)
    x_test_pd = pd.DataFrame(x_test_np)

    print("{} dimensions after JL dimensionality reduction".format(dim))

    # build model
    model = CNN_JL(int(np.floor((np.floor((dim-4)/2)-4)/2)*100)).cuda()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())

    train_loss = []
    interval_tuples = []
    start = time.time()

    pcap_train.x_train = torch.tensor(x_train_pd.values)
    pcap_test.x_train = torch.tensor(x_test_pd.values)

    train_loader = torch.utils.data.DataLoader(pcap_train, batch_size=25, shuffle=True, drop_last=True)
    test_loader = torch.utils.data.DataLoader(pcap_test, batch_size=25, shuffle=False, drop_last=True)

    for epoch in range(5):                               # todo - change back to 5 when running
        running_train_loss = 0.0
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data
            labels = labels.long()
            inputs = np.expand_dims(inputs, axis=1)
            inputs = torch.Tensor(inputs)
            inputs = inputs.cuda() # -- for GPU
            labels = labels.cuda() # -- for GPU
            
            # zero the parameters gradients
            optimizer.zero_grad()

            # forward + backward + optimization
            outputs = model(inputs)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            running_train_loss += loss.item()
            if (i+1) % 100 == 0:
                interval_tuples.append(str((epoch + 1, i + 1)))
                print("[{}, {}] loss: {}".format(epoch + 1, i + 1, running_train_loss / 100))
                train_loss.append(running_train_loss / 100)
                running_train_loss = 0.0

    stop = time.time()
    print("Training time: {}".format(stop-start))

    # test accuracy
    total = 0
    correct = 0
    with torch.no_grad(): 
        for data in test_loader:
            inputs, labels = data
            labels = labels.float()
            inputs = np.expand_dims(inputs, axis=1)
            inputs = torch.Tensor(inputs)
            inputs = inputs.cuda() # -- for GPU
            labels = labels.cuda() # -- for GPU

            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = (100 * correct / total)
    acc_jl_list.append(accuracy)
    print('Accuracy: {}%'.format(accuracy))

# create the figure
plt.figure(figsize=[12,8])
plt.title("Accuracy  after JL dimensionality reduction")
plt.xticks(components)
plt.xlabel("Number of dimensions")
plt.ylabel("Accuracy")
plt.ylim((79.5,85))

# plot the baseline and random projection accuracies
plt.plot(components, acc_jl_list)
plt.hlines(93.47, 0.01, 0.5, linestyle='dashed', color='r')

# dimensionality reduction with PCA

df_train = pd.DataFrame(pcap_train.x_train.numpy())
df_test = pd.DataFrame(pcap_test.x_train.numpy())

from sklearn.decomposition import PCA

acc_pca_list = []
from sklearn.random_projection import SparseRandomProjection
# train model and get predictions
for dim in components:
    # dimensionality reduction
    # create the transformation
    pca = PCA(n_components = dim)
    x_train_np = pca.fit_transform(df_train)
    x_train_pd = pd.DataFrame(x_train_np)

    # evaluate the model and update the list of accuracies
    x_test_np = pca.transform(df_test)
    x_test_pd = pd.DataFrame(x_test_np)

    print("{} dimensions after PCA dimensionality reduction".format(dim))

    # build model
    model = CNN_JL(int(np.floor((np.floor((dim-4)/2)-4)/2)*100)).cuda()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())

    train_loss = []
    interval_tuples = []
    start = time.time()

    pcap_train.x_train = torch.tensor(x_train_pd.values)
    pcap_test.x_train = torch.tensor(x_test_pd.values)

    train_loader = torch.utils.data.DataLoader(pcap_train, batch_size=25, shuffle=True, drop_last=True)
    test_loader = torch.utils.data.DataLoader(pcap_test, batch_size=25, shuffle=False, drop_last=True)

    for epoch in range(5):
        running_train_loss = 0.0
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data
            labels = labels.long()
            inputs = np.expand_dims(inputs, axis=1)
            inputs = torch.Tensor(inputs)
            inputs = inputs.cuda() # -- for GPU
            labels = labels.cuda() # -- for GPU
            
            # zero the parameters gradients
            optimizer.zero_grad()

            # forward + backward + optimization
            outputs = model(inputs)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            running_train_loss += loss.item()
            if (i+1) % 100 == 0:
                interval_tuples.append(str((epoch + 1, i + 1)))
                print("[{}, {}] loss: {}".format(epoch + 1, i + 1, running_train_loss / 100))
                train_loss.append(running_train_loss / 100)
                running_train_loss = 0.0

    stop = time.time()
    print("Training time: {}".format(stop-start))

    # test accuracy
    total = 0
    correct = 0
    with torch.no_grad(): 
        for data in test_loader:
            inputs, labels = data
            labels = labels.float()
            inputs = np.expand_dims(inputs, axis=1)
            inputs = torch.Tensor(inputs)
            inputs = inputs.cuda() # -- for GPU
            labels = labels.cuda() # -- for GPU

            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = (100 * correct / total)
    acc_pca_list.append(accuracy)
    print('Accuracy: {}%'.format(accuracy))

# create the figure
plt.figure(figsize=[12,8])
plt.title("Accuracy after PCA dimensionality reduction")
plt.xlabel("Number of dimensions")
plt.ylabel("Accuracy")
# plt.ylim((79.5,85))

# plot the baseline and random projection accuracies
plt.plot(components, acc_pca_list)
plt.plot(components, acc_jl_list)
plt.legend()
plt.hlines(93.47, 0.01, 0.5, linestyle='dashed', color='r')